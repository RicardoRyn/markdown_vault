之前的`recon-all`命令用了`qcache`选项，这会生成不同平滑度（例如0mm、5mm、10mm、15mm、20mm、25mm平滑核）下的厚度、体积、弯曲率的map

基于surface的分析有一点好处，就是可以使用更大的平滑核（smoothing kernels），

![../../_images/08_Qcache_Output.png](https://andysbrainbook.readthedocs.io/en/latest/_images/08_Qcache_Output.png)

# 用mris_preproc创建组文件

为了进行组分析，需要将所有被试的structural maps合并到一个数据集中去，就像fMRI把连续的全脑合在一个dataset里一样

![../../_images/08_mrispreproc_Concatenation.gif](https://andysbrainbook.readthedocs.io/en/latest/_images/08_mrispreproc_Concatenation.gif)

上图中一个图属于一个被试，我们还要根据fsaverage模板**重采样（resample）**到MNI空间；反正无论何种组分析，每个被试都必须具有相同的维度和voxel分辨率

使用`mris_preproc`命令对每个被试进行**重采样**，这条命令需要4个参数：

1. 需要FSGD文件，`--fsgd`
2. 需要一个模板，`--target`
3. 需要重采样哪个半脑，`--hemi`
4. 输出文件名，`--out`

在本例中，我们还需要使用`--cache-in`来指定我们需要使用何种平滑的图片（smoothed images）；之前的`-recon-all`中已经用`--qcache`生成了不同平滑度的图像，任何一种都可以选择



脚本`runMrisPreproc.sh`如下：**（要键入`tcsh runMrisPreproc.sh CannabisStudy`来运行）**

```bash
#!/bin/tcsh

setenv SUBJECTS_DIR `pwd`  # This line may be invalid, and you may need to change the SUBJECTS_DIR manually in terminal before run. By RJX

setenv study $argv[1]

foreach hemi (lh rh)
  foreach smoothing (10)
    foreach meas (volume thickness)
      mris_preproc --fsgd FSGD/{$study}.fsgd \
        --cache-in {$meas}.fwhm{$smoothing}.fsaverage \
        --target fsaverage \
        --hemi {$hemi} \
        --out {$hemi}.{$meas}.{$study}.{$smoothing}.mgh
    end
  end
end
```

结果会生成8个文件：

1. `lh.thickness.CannabisStudy.10.mgh`
2. `lh.thickness.CannabisStudy.10.mris_preproc.log`
3. `lh.volume.CannabisStudy.10.mgh`
4. `lh.volume.CannabisStudy.10.mris_preproc.log`
5. `rh.thickness.CannabisStudy.10.mgh`
6. `rh.thickness.CannabisStudy.10.mris_preproc.log`
7. `rh.volume.CannabisStudy.10.mgh`
8. `rh.volume.CannabisStudy.10.mris_preproc.log`

试了一下`freeview`打不开

# 用mri_glmfit拟合一般线性模型

现在所有被试的数据都被放入**同一个数据集（concatenated dataset）**中

接下来使用mri_glmfit拟合一般线性模型，需要以下参数：

- 该数据集包含所有被试的结构map，`--y`
- FSGD文件，`--fsgd`
- 对比列表，每一行代表特定的contrast，`--C`
- 模板的半脑用来分析，`--surf`
- 用来限制只在皮层中分析的mask，`--cortex`
- 装结果的文件夹的名字（label），`--glmdir`

本例中，我们将要分析平滑核为10mm的数据，包括左右半脑，会分析体积和厚度

脚本`runGLMs.sh`如下：

```bash
#!/bin/tcsh

set SUBJECTS_DIR = `pwd`  # This line may be invalid, and you may need to change the SUBJECTS_DIR manually in terminal before run. By RJX

set study = $argv[1]

foreach hemi (lh rh)
  foreach smoothness (10)
    foreach meas (volume thickness)
        mri_glmfit \
        --y {$hemi}.{$meas}.{$study}.{$smoothness}.mgh \  # mris_preproc生成的文件
        --fsgd FSGD/{$study}.fsgd \
        --C Contrasts/CB-HC.mtx \
        --C Contrasts/HC-CB.mtx \
        --surf fsaverage {$hemi}  \
        --cortex  \
        --glmdir {$hemi}.{$meas}.{$study}.{$smoothness}.glmdir  # 最后输出的文件名字
    end
  end
end
```

# 查看输出结果

如果没有任何错误，会看到以下文件：

```txt
lh.thickness.CannabisStudy.10.glmdir
lh.volume.CannabisStudy.10.glmdir
rh.thickness.CannabisStudy.10.glmdir
rh.volume.CannabisStudy.10.glmdir
```

文件名分别表示：左右半脑，测量的结构，研究名字，平滑核；文件中内容如下：

![../../_images/08_FreeSurfer_GroupAnalysis_Directory.png](https://andysbrainbook.readthedocs.io/en/latest/_images/08_FreeSurfer_GroupAnalysis_Directory.png)

> 蓝色的`CB-HC`和`HC-CB`是文件夹
>
> `y.fsgd`是之前的FSGD文件的拷贝
>
> `mri_glmfit.log`包含了当前分析用的脚本
>
> `mask.mgh`是分析用的mask；`beta.mgh`是一个连续的数据集（concatenated dataset），把每个被试的beta权重连在一起

可以通过键入`mri_glmfit`来查看其他选项的信息

---

在每个contrast文件中可以看到以下内容：

![../../_images/08_ContrastDirectoryContents.png](https://andysbrainbook.readthedocs.io/en/latest/_images/08_ContrastDirectoryContents.png)

> 上述的beta数据集 × contrast权重 得到 `gamma.mgh`文件；这个文件可以显示，例如：对照组在左半脑的哪个位置上的volume大于大麻组
>
> `z.mgh`文件将上述对比文件转换成z-map
>
> `sig.mgh`文件将上述对比文件转换成p值；注意FreeSurfer里面用`-log10(p)`，例如1就表示p值是0.1，2表示p值是0.01

可以通过键入`mri_glmfit`来查看其他选项的内容

---

为了在fsaverage模板上呈现统计map，可以定位到任意contrast文件夹（例如`lh.volume.CannabisStudy.10.glmdir/HC-CB`）中键入：

```bash
freeview -f $SUBJECTS_DIR/fsaverage/surf/lh.inflated:overlay=sig.mgh
```

![../../_images/08_sigMGH_Overlay.png](https://andysbrainbook.readthedocs.io/en/latest/_images/08_sigMGH_Overlay.png)

> 阈值的p值可以通过上述红框里的`Configure`来调整

至此，我们的结果已经基本完成，但是为了确定这些显著的结果不是false positives，我们还要做**多重比较校正**，即**cluster correction**
