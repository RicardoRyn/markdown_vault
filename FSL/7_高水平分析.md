# 2nd_Level Analysis

afni和SPM里都将`2nd-level analysis`定义为**组分析**

但fsl中，`2nd-level analysis`是对从`1st-level analysis`中得到的每个被试的**parameter estimate（β值，pe's）**和**contrast estimate（contrast of β值，cope's）**做平均

应该是指：每个被试都有很多的voxel，每个voxel都会拟合出一个GLM，即每个体素都有各自的pe、有各自的cope，然后对所有voxel做平均

> In FSL a 2nd-level analysis is the averaging together within each subject the parameter estimates and contrast estimates from the 1st-level analyses

下面的`Data` tab中有2个选项：

1. `Inputs are lower-level FEAT directories`
2. `Inputs are 3D cope images from FEAT directories`

选择第一个选项需要之前的分析都是由fsl默认的处理流（processing stream）进行的

选择第二个选项可以更灵活地进行分析

## 选择FEAT文件夹

26个被试，各2个run，共有52个FEAT directory

需要将FEAT里`Data` tab中的`Number of inputs`更改成52，然后点击`Select FEAT directories`按钮，如果手动输入慢且容易出错

回到打开FEAT的终端，按下`Ctrl`加`z`，将FEAT程序挂起，然后键入`bg`，让后台挂起的命令继续执行

> 通过bg %num 即可将挂起的job的状态由stopped改为running，仍在后台执行；当需要改为在前台执行时，执行命令fg %num即可

然后键入：

```bash
ls -d $PWD/sub-??/run*
```

将终端中打印出的列表复制到剪贴板（clipboard）

重新回到`Select input data`窗口中，点击下方的`Paste`，并在新的窗口中按下`Ctrl`加`y`，将剪贴板的内容粘贴出来，点击`OK`，完成52个FEAT directories的输入

![../_images/2ndLevelAnalysis_SelectingFEATDirectories.png](https://andysbrainbook.readthedocs.io/en/latest/_images/2ndLevelAnalysis_SelectingFEATDirectories.png)

---

`Data` tab中有3个lower-level copes，分别对应：

1. incongruent的cope
2. congruent的cope
3. incongruent的cope减去congruent的cope

在`Output directory`中填写

## 创建GLM

点击`Stats` tab，和1st-level analysis不同，现在可以选择不同的inference类型，即如何将结果泛化到population

第一个下拉菜单里有5个选项：

1. `Fixed Effects`：不从样本中泛化，只做平均
2. `Mixed Effects: Simple OLS`：普通最小二乘法 （Simple Ordinary Least Squares）；对每个被试的平均pe做t检验，不考虑每个被试的各个run之间的变化
3. `Mixed Effects: FLAME 1`：根据cope的方差（variance）对每个被试的pe进行加权；即一个被试的方差如果更低，则拥有更大的权重，如果方差更大，则拥有更小的权重
   FLAME 1 (FSL’s Local Analysis of Mixed Effects)，利用被试内和被试间的变动（variability）的信息提供了精确的pe
4. `Mixed Effects: FLAME 1+2`：FLAME 1增强版，需要时间更长，只对分析小样本数据有帮助（例如10个被试或者更少）
   相比于FLAME 1更加精确，但是增加的收益很少，还费时间
5. `Randomise`：非参数检验（后面再说）
   当不服从正态分布时，才考虑用这个

本例中，我们只想取每个被试在不同run间的的pe的平均值，所以选择第一个`Fixed Effects`

---

然后点击`Full Model Setup`，弹出一个新的窗口

其中的行数表示每个个体的参数估计的个数，本例中有52个，对应之前输入的52个FEAT directory文件；即一个被试有2个run，说明有2个参数估计，而26个被试共有52个参数估计

将`Number of main EVs`更改为26，即被试数

将每个被试（EV1~EV26）对应的2个参数估计（Input1~Input52）设为1，即说明对于该被试，你想要取这2个参数估计的平均

![../_images/2ndLevelAnalysis_GLM_Setup.png](https://andysbrainbook.readthedocs.io/en/latest/_images/2ndLevelAnalysis_GLM_Setup.png)

然后点击`Contrast & F-tests` tab，将`contrasts`更改为26

然后把对角线上的值都更改为1，表明为每个被试创建单个的cope，也即pe的平均

![../_images/2ndLevelAnalysis_Contrast_Setup.png](https://andysbrainbook.readthedocs.io/en/latest/_images/2ndLevelAnalysis_Contrast_Setup.png)

然后会弹出下图

![../_images/2ndLevelAnalysis_Model.png](https://andysbrainbook.readthedocs.io/en/latest/_images/2ndLevelAnalysis_Model.png)

和1st-level analysis一样，先不管`Post-stats` tab

# 3rd-Level Analysis

这一水平的分析是想将样本里发现的效应，泛化到整个population

在fsl中，3rd-Level Analysis才是**组分析**

在这一步里，需要计算**标准误差**和**cope的平均**，然后检测平均的estimate是否具有统计显著性

---

fsl一次只能跑一个model，在本例中，将会跑`Incongruent-Congruent`的cope（标签为`cope3`）

在Flanker文件夹中打开fsl的FEAT GUI，选择`Higer-level analysis`，与2nd-level analysis不同的是，`Data` tab下面选择`Input are 3D cope images from FEAT directories`

我们有26个被试，每个被试都有一个cope3文件，所以将`Number of inputs`更改为26

然后点击`Select cope images`，把所有被试的cope3文件输进去

和之前一样，在终端按下ctrl加z，然后键入bg，然后键入：

```bash
cd Flanker_2ndLevel.gfeat/cope3.feat/stats
ls $PWD/cope* | sort -V  
# sort用于将打印出的内容排列成一整列（此时仍可能是乱序），-V只在文本内进行自然版本排序，即按123的顺序排序，|管道符号表示前一条命令的输出作为后一条命令的输入；注意这样排序的结果10号被试是否在1号被试前面，如果是，请手动调整
```

然后利用`ctrl`加`y`粘贴进去，注意这里填写的内容是绝对路径，所以一定要打印`$PWD`否则会提示`ERROR`

---

然后进入`Stats` tab，我们将会用`Mixed Effects`（有3个选项），因为它对方差（variance）进行了建模，能将结果泛化到population上去，本例中选择`Mixed Effects: FLAME 1`

因为我们使用的时简单设计，所以我们能通过`Model setup wizard`快速创建一个GLM；我们早已设置好了每个被试的contrast，所以点击`single group average`

---

进入`Post-Stats` tab，唯一需要注意的就是`Thresholding`里的选项

下拉菜单中有4个选项：

1. None：表示不卡任何阈值，即展现每个voxel的pe，不管显不显著
2. Uncorrected：允许那些通过了Z-threshold的voxel（本例中卡的Z threshold是3.1）
3. Voxel：会应用一种基于高斯随机场理论（Gaussian Random Field theory）的最大高度阈值；已经很保守了但没有Bonferroni保守
4. Cluster：使用CDT（cluster-defining threshold）来决定一个cluster是否显著，其背后的逻辑是，一个cluster里的voxel并不是相互独立的，在估计显著性时，减少了其自由度

![../_images/3rdLevelAnalysis_PostStatsTab.png](https://andysbrainbook.readthedocs.io/en/latest/_images/3rdLevelAnalysis_PostStatsTab.png)

如果我们选择`Cluster`，`Z-threshold`选择3.1，`Cluster P threshold`选择0.05，我们会看到由那些通过3.1 `Z-threshold`的voxel构成的cluster；fsl会使用模拟数据，看这些卡过3.1的voxel组成的有明确大小（size）的cluster有多少概率通过预设的`Cluster  P threshold`（置换检验思想）

> 大部分实验，选择3.1和0.05就够了；
>
> z-score，z分数，zstat，标准分数，（不清楚z-score和z-value之间是不是指同一件事，但是我还是把它俩混用；但是z转换，z-transformation好像不是同一回事）；z-value刻画了一个数值在一批数值中的相对位置。
>
> ![img](https://bkimg.cdn.bcebos.com/pic/d058ccbf6c81800aaff74971b13533fa828b47bb?x-bce-process=image/resize,m_lfit,w_164,limit_1/format,f_auto)

![../../_images/Zstat_Peak_Demo.gif](https://andysbrainbook.readthedocs.io/en/latest/_images/Zstat_Peak_Demo.gif)

最后点击`Go`，开始跑3rd-level analysis

---

最后打开fsleyes

打开`/usr/local/fsl/data/standard`文件夹中的MNI152_T1_1mm_brain模板图作为underlay

然后打开`Flanker_3rdLevel_inc-con.gfeat/cope1.feat`下的thresh_zstat1.nii.gz作为overlay

为了让结果更清楚，可以选择overlay，然后将Min更改为3.1

