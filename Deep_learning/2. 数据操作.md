# 张量是什么？

在计算机深度学习的语境下，张量就是指n维数组

因为计算中，张量更多指数据结构：
1. 标量：0维数组（单个数）
2. 向量：1维数组
3. 矩阵：2维数组
4. 更高维数组（比如3维、4维）也称为张量

# 节省内存小技巧

```python
# 以下新生成的Y和旧的Y，之间的内存地址不一致，说明新分配内存了，浪费。
Y = X + Y

# 节省例子1:
Y += X  # 没有新分配内存，节省。

# 节省例子2:
Z = torch.zeros_like(Y)
print("id(Z):", id(Z))
Z[:] = X + Y
print("id(Z):", id(Z))  # 前后一致
```

# Hadamard积

两个矩阵的按元素乘法称为$Hadamard积$（Hadamard product）（数学符号$⊙$）

# 范数

从根本上讲，**范数是用来衡量“向量有多大”或“向量之间有多远”**的数学工具。

非正式的说，范数就是向量所表征的线段的长度。

它的意义体现在几个方面：
1. 量化距离和大小
2. 让空间具有结构

范数必须满足3个性质:
1. $f( \alpha x)=|\alpha|f(x)$：如果我们按常数因子$\alpha$缩放向量的所有元素， 其范数也会按相同常数因子的*绝对值*缩放
2.  $f(x+y) \le f(x) + f(y)$：三角不等式
3.  $f(x)\gt 0$：范数必须是非负的

---

$L_1$范数也就是曼哈顿距离，即各个防线上的绝对值的总和。

其中，在$L_2$范数中常常省略下标$2$，也就是说$\|\mathbf{x}\|$等同于$\|\mathbf{x}\|_2$。

为什么有这么多种范数？因为不同的范数在不同的问题中更合适、更有意义。

---

类似于向量的$L_2$范数，**矩阵**$\mathbf{X} \in \mathbb{R}^{m \times n}$(**的*Frobenius范数*（Frobenius norm）是矩阵元素平方和的平方根：**
$$\|\mathbf{X}\|_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n x_{ij}^2}.$$
---

为啥深度学习中老是提到范数？

在深度学习中，我们经常试图解决优化问题： 
1. 最大化分配给观测数据的概率; 
2. 最小化预测和真实观测之间的距离。 

用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离，这也就是我们的目标。

**目标**，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。

# 梯度

梯度是一个向量，其分量/元素是多变量函数相对于其所有变量的偏导数。

```python
x = torch.arange(4.0)
x.requires_grad_(True)  # 告诉 PyTorch：“我要对 `x` 进行**自动求导**，请开始追踪它的梯度”
# 上面代码等价于x=torch.arange(4.0,requires_grad=True)
x.grad  # 默认值是None

```

# 为什么pytorch中的反向传播算法要求标量函数

当 y 是非标量张量（如向量、矩阵）时，PyTorch 不知道你想对哪个方向进行反向传播，因此必须提供一个“权重向量” gradient 参数，告诉它如何把 y “合成”成一个标量后再反向传播。

如果有一个标量函数 $y=f(x)$，则他对$x$的导数$\frac{dy}{dx}$是明确定义的

但如果$y$是一个向量函数$y=[y_1,y_2,y_3]$，那导数就不是单个导数了，而是一个**Jacobian矩阵**

**Jacobian 矩阵**是一个向量函数对多个变量的所有偏导数组成的矩阵
假设：
$$ \mathbf{y} = 
\begin{bmatrix}
y_1(x_1, x_2) \\
y_2(x_1, x_2)
\end{bmatrix}
=
\begin{bmatrix}
x_1^2 + x_2 \\
\sin(x_1) \cdot x_2
\end{bmatrix}
$$
 对应的Jacobian矩阵为：
 $$
J = 
\begin{bmatrix}
\frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} \\
\frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2}
\end{bmatrix}
=
\begin{bmatrix}
2x_1 & 1 \\
\cos(x_1) \cdot x_2 & \sin(x_1)
\end{bmatrix}
 $$
由于$y$是多个输出对多个输入，就不能用一个普通导数表示，而是用 Jacobian 矩阵
 
 