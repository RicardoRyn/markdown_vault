# 协变量

协变量也是变量，深度学习中的各种特征，也叫做协变量

# 偏置

$$\mathrm{price} = w_{\mathrm{area}} \cdot \mathrm{area} + w_{\mathrm{age}} \cdot \mathrm{age} + b.$$
$b$称为*偏置*（bias）、*偏移量*（offset）或*截距*（intercept）。
偏置是指当所有特征都取值为0时，预测值应该为多少。
即使现实中不会有任何房子的面积是0或房龄正好是0年，仍然需要偏置项。
如果没有偏置项，模型的表达能力将受到限制。

# 仿射变换

*仿射变换*（affine transformation）。
仿射变换的特点是通过加权和对特征进行*线性变换*（linear transformation），
并通过偏置项来进行*平移*（translation）。

# 再次理解L2范数

一个向量的 $L_2范数的平方$，就是这个向量所有元素的平方和。
所以看到平方和，就可以联想范数

# 梯度下降

梯度下降最简单的用法是计算损失函数（即数据集中所有样本的损失均值）
关于模型参数的导数（在这里也可以称为梯度）。

但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。
因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本，
这种变体叫做*小批量随机梯度下降*（minibatch stochastic gradient descent）。

# 似然

概率（Probability）是给定参数，计算数据出现的可能性。
似然（Likelihood）是给定数据，评价参数值的合理性。

假设我们有硬币正面朝上的次数数据$D$:
- **概率** 是给定硬币“正面概率”参数 $\theta$ 时，计算出现这个数据的概率 $P(D|\theta)$。
- **似然** 则是用这个数据来估计硬币正面概率的合理值 $L(\theta|D)$。

在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计

# python中的yeild关键字

是一个用于定义**生成器函数**的关键字

# 线性回归的从零开始实现

## 1. 小批量读取数据`data_iter`

一个小批量读取数据的函数`data_iter`，这个函数往往是个生成器函数。
一般会使用循环来调用这个函数，
每次循环都动态生成数据

## 2. 初始化参数`w`, `b`，定义模型`linreg`

初始化参数，如果参数初始化足够离谱，在有限次数的学习之下，学得稀烂

## 3. 定义损失函数`squared_loss`，定义优化算法`sgd`

#