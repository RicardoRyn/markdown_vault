# 熵

**熵**：信息论中的熵是**一种衡量“信息不确定性/随机性”的度量**。它可以**用来描述一个随机变量的不确定性**，或者**一个信息源产生信息的平均速率**。单位是bits，能够表示信息量的多少。

熵的数学定义如下：

对于一个离散随机变量X，其可能的取值集合为$\{x_1,x_2,…,x_n\}$，每个取值$x_i$的概率为$p(x_i)$，则随机变量X的熵$H(X)$定义为：
$$
H(X) = -\sum_{i=1}^{N} p(x_i)log~p(x_i)
$$
其中，$log$⁡是以2为底的对数，用于将熵的单位表示为比特（bits），**此时熵解释为：“我们编码信息所需要的最小比特数”**。

以下是熵的一些关键性质和解释：

1. **不确定性的度量**：熵越高，随机变量的不确定性越大。如果所有可能的取值都等概率发生，那么熵将达到最大值。
2. **信息的平均量**：熵可以解释为从该分布产生的每个符号或事件的平均信息量。
3. **非负性**：熵总是非负的，最小值为0，当且仅当随机变量完全确定时（即其中一个取值的概率为1，其余取值的概率为0）。
4. **增加的不确定性**：如果有两个或更多的独立随机变量，整体系统的熵等于各个随机变量熵的和。

# 概率，信息

**概率**：概率是一个数学概念，用于描述随机事件发生的可能性。概率的值在0和1之间，其中0表示事件不可能发生，1表示事件一定会发生。**概率为我们提供了一种量化不确定性的方法**。

**信息**：**信息是对事物的描述或解释**。在信息论中，信息与不确定性有关。**一个随机事件的信息量与其发生的概率有关**。**具体来说，事件的信息量与其概率的负对数成正比**。一个不太可能发生的事件具有更高的信息量，而一个更可能发生的事件具有较低的信息量。信息的单位通常是**“比特”（bit）**。比特是二进制数字系统的基本单位，可以表示两个可能的状态之一，通常表示为0或1。

# 例子

假设一个袋子里有6个球，一号球1个，二号球2个，三号球3个

现在随机抽一个球，抽到一号球的事件命名为$x_1$，抽到二号球的事件命名为$x_2$，抽到三号球的事件命名为$x_3$

$x_1$发生的概率为$p(x_1) = 1/6$

$x_2$发生的概率为$p(x_2) = 2/6$

$x_3$发生的概率为$p(x_3) = 3/6$

事件$X$的熵为：
$$
\begin{align*}
H(X) &= -\sum_{i=1}^{N} p(x_i)log~p(x_i) \\
&= -(p(x_1)log~p(x_1) + p(x_2)log~p(x_2) + p(x_3)log~p(x_3)) \\
&= -(\frac{1}{6}\times log\frac{1}{6} + \frac{2}{6}\times log\frac{2}{6} + \frac{3}{6}\times log\frac{3}{6}) \\
&= 1.459
\end{align*}
$$
我们编码事件$X$所需要信息的最小比特数为2（因为比特数为整数）

# KL散度 (KL divergency)

## 数学期望

数学期望（或称为期望值、均值、平均值）是概率论和统计学中的一个基本概念。

它提供了随机变量可能取值的“平均”或“中心”水平。对于离散随机变量，数学期望定义为其每个可能取值与相应概率的乘积之和。

---

设随机变量 $x_i$，有两个概率分布 $P$ 、 $Q$其中， $P$ 为我们的真实概率分布， $Q$ 为我们简化 $P$ 时候用的近似分布（预测分布）

当 $x_i$ 为离散型随机变量时候，定义从 $P$ 到 $Q$ 的KL散度为：
$$
D_{KL}(P||Q) = \sum_{i}^{N}p(x_i)\times(log~p(x_i)-log~q(x_i)) = \sum_{i}^{N}p(x_i)\times(log~\frac{p(x_i)}{q(x_i)})
$$
如果 xi 为连续随机变量，则定义从 P 到 Q 的KL散度为：
$$
D_{KL}(P||Q) = \int_{-\infty}^{\infty}p(x_i)\times(log~p(x_i)-log~q(x_i))
$$
本质上，我们用KL散度看的是**对原始分布中的数据概率与近似分布之间的对数差的期望**。

因此，我们可以根据期望重写KL的公式：
$$
D_{KL}(P||Q) = E[log~p(x_i) - log~q(x_i)]
$$

## 怎么理解KL散度

KL散度又被称之为“相对熵”，说的是：如果我们用分布$Q$来近似真实分布$P$，需要付出的额外信息。

$P(x)$反映了事件$x$的实际发生可能性，$Q(x)$反映了我们对事件$x$的预测可能性

$log\frac{P(x)}{Q(x)}$描述了$Q(x)$相较于$P(x)$的偏差，

- 如果$Q(x)$和$P(x)$完全一致，则$log\frac{P(x)}{Q(x)}=0$
- 如果$Q(x)$偏离了$P(x)$，则该值表示偏差的严重程度
- 在信息论中，$-logP(x)$表示描述事件$x$所需的信息量。用$Q(x)$描述时的**额外信息代价**就是$-logQ(x)-(-logP(x))=log\frac{P(x)}{Q(x)}$

$P(x)log\frac{P(x)}{Q(x)}$表示对所有可能的$x$加权求和（或者积分），表示整体上的期望信息损失，即KL散度。

