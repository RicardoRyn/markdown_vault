# 零、回归 与 相关

1组值就可以求：

1. **标准差**/**均方差** SD, standard deviation：$\sqrt{\frac{\sum{(a_i-\bar a)^2}}{n}}$
2. **方差** variance, Var：$\frac{\sum{(a_i-\bar a)^2}}{n}$
3. **均方根** RMS, root mean square：$\sqrt{\frac{\sum{a_i^2}}{n}}$
   当一组数的均值是0时，**标准差**=**均方根**

2组值才能求：

1. **误差**/**残差** error：$a_i-b_i$
2. **偏差**：$a_i-\bar{a}$
3. **协方差** covariance, Cov：$\frac{\sum{(a_i-\bar a)(b_i-\bar b)}}{n}$
4. **均方误差** MSE, Mean Squared Error：$\frac{\sum{(a_i-b_i)^2}}{n}$



**回归**和**相关**是统计学和数据分析中两个不同的概念，它们用于不同的分析和推断目的。

1. **回归（Regression）**：
   - 回归分析是一种用于研究变量之间关系的统计方法。它通常用于预测一个或多个因变量（目标变量）如何受一个或多个自变量（解释变量）的影响。
   - 回归分析的目标是建立一个数学模型，该模型可以描述因变量和自变量之间的关系，使我们能够进行预测或推断。
   - 常见的回归方法包括线性回归、多元线性回归、逻辑回归、岭回归等。这些方法用于处理不同类型的数据和问题。
2. **相关（Correlation）**：
   - 相关是一种统计方法，用于测量两个或多个变量之间的关联程度。它衡量了这些变量之间的线性关系的强度和方向。
   - 相关系数通常在-1到1之间取值，-1表示完全负相关，1表示完全正相关，0表示没有线性关系。
   - **相关分析并不涉及因果关系，仅表明变量之间的关系程度。**例如，如果两个变量相关，那么它们可能一起增加或减少，但我们不能确定其中一个是因为另一个而发生的。

总结来说，回归用于建立模型以**预测或解释因变量与自变量之间的关系**，而相关用于**测量变量之间的关联程度，但不提供因果关系信息**。



# 一、皮尔逊相关

计算公式：
$$
r(x,y)=\frac{Cov(x,y)}{\sigma_x \sigma_y} \\
= \frac{\sum{(x_i-\bar x)(y_i-\bar y)}}{\sqrt{\sum{(x_i-x)^2}}\sqrt{\sum{(y_i-y)^2}}}
$$




# 一、线性回归

目标：将数据一分为二

如果数据是二维空间的点，就找出一条线将这些点一分为二

如果数据是三维空间的点，就找出一个平面将这些点一分为二

如果数据是高位空间的点，就找出一个超平面将这些点一分为二

在线性回归中，一般用一个**行向量**来表示一个数据点，这与深度学习不一样。

现在有5个数据点，每个数据都是3个维度，即$x_{mn},m=5,n=3$，其中任意一个称为：第$i$个数据，第$j$个维度

$$
X=\left(
\begin{matrix}
x_{11} & x_{12} & x_{13} \\
x_{21} & x_{22} & x_{23} \\
x_{31} & x_{32} & x_{33} \\
x_{41} & x_{42} & x_{43} \\
x_{51} & x_{52} & x_{53} \\
\end{matrix}
\right)
$$
每个数据都对应一个**标签**$y$，则把数据点和标签对应在一起就有：

**注意**：这里标签$y$是标签，不是因变量，硬要说因变量可以是$x_{i3}$

**注意**：这里标签$y$是标签，不是因变量，硬要说因变量可以是$x_{i3}$

**注意**：这里标签$y$是标签，不是因变量，硬要说因变量可以是$x_{i3}$
$$
(X,y)=\left[
\begin{matrix}
x_{11} & x_{12} & x_{13} & y_1 \\
x_{21} & x_{22} & x_{23} & y_2 \\
x_{31} & x_{32} & x_{33} & y_3 \\
x_{41} & x_{42} & x_{43} & y_4 \\
x_{51} & x_{52} & x_{53} & y_5 \\
\end{matrix}
\right]
=(X_1,X_2,X_3,X_4,X_5,y)^T
$$
再次强调，上述矩阵**一行是一个数据**，与深度学习/神经网络不一样

# 二、线性回归的表示

再次复习：线性回归的目标是寻找一个超平面来拟合上述数据，使得各个数据点的残差平方和最小，该平面为：
$$
\hat y
=
\left(
\begin{matrix}
x_{13} \\
x_{23} \\
x_{33} \\
x_{43} \\
x_{53} \\
\end{matrix}
\right)
=
\left(
\begin{matrix}
\beta_1x_{11}+\beta_2x_{12}+\beta_0 \\
\beta_1x_{21}+\beta_2x_{22}+\beta_0 \\
\beta_1x_{31}+\beta_2x_{32}+\beta_0 \\
\beta_1x_{41}+\beta_2x_{42}+\beta_0 \\
\beta_1x_{51}+\beta_2x_{52}+\beta_0 \\
\end{matrix}
\right)
=
\left(
\begin{matrix}
x_{11} & x_{12} \\
x_{21} & x_{22} \\
x_{31} & x_{32} \\
x_{41} & x_{42} \\
x_{51} & x_{52} \\
\end{matrix}
\right)
\left(
\begin{matrix}
\beta_{1} & \beta_{2} \\
\end{matrix}
\right)^T
+\beta_0
=
X\beta^T+\beta_0
$$
**注意**：这里$\hat y$不是标签，是因变量，可以看成$x_{i3}$

**注意**：这里$\hat y$不是标签，是因变量，可以看成$x_{i3}$

**注意**：这里$\hat y$不是标签，是因变量，可以看成$x_{i3}$

# 三、最小二乘 Least squares

**最小二乘**，就是将**二乘最小化**。

**二乘**是什么，简单来说就是**残差的平方和**。

**残差**是什么，就是**预测值**和**真实值**之间的差

## 线性最小二乘 Linear Least Squares

分成3类：

- 普通最小二乘法（Ordinary Least Squares, OLS）
- 加权最小二乘法（Weighted Least Squares, WLS）
- 广义最小二乘法（Generalized Least Squares, GLS ）

参看线性回归线性模型的形式可以写成：
$$
\hat y=X\beta^T \\
$$

一般情况下这个方程是无解的，我们希望能找到一个最优的β可以最好的适配数据：
$$
\hat \beta=argmin\,S(\beta) \\
argmin是一个函数名，后面跟一个函数f(x)，表示使f(x)取到最小值时的变量值
$$
而$S(\beta)$是我们的目标函数，$S(\beta)$最小也就是想让**残差平方和最小**：
$$
S(\beta)=\sum_{i=1}^m|y_i-\hat y_i|^2=||y-X\beta^T||^2 \\
y_i表示真实值（单个） \\
\hat y_i表示预测值（单个） \\
y表示真实值（一组）
$$

### 1. 普通最小二乘 OLS

目标：求得一个使得**全局残差平方和**最小的参数

**残差/误差满足Gauss-Markov定理**

> 误差/残差：一般认为在某程度上具有很大的相似性，都是衡量不确定性的指标，可是两者又存在区别。简单理解为**误差与测量有关，残差与预测有关**。
>
> 下面的**误差/残差**统称为**残差**

**Gauss-Markov定理**：在线性回归模型中，如果残差满足零均值、同方差且互不相关，则回归系数的最佳线性无偏估计(**BLUE**, Best Linear unbiased estimator)就是普通最小二乘法估计。

3个理解点：

1. 残差满足零均值。（好理解，不赘述，也可以说成**残差**的**数学期望**为0）
2. 残差满足同方差。（指各个维度上的残差都服从均值为0，方差为$\sigma^2$的正态分布）
3. 残差互不相关。（最难理解的一点。其实是指**同一维度下的不同数据点的残差**和**自变量**互不相关，**互不相关**也就是**协变量为0**的意思；也就是对于**点A**，它的残差**可以很高**，**也可能很低**，**完全是随机的**；对于点B，它的残差也可能很高，也可能很低，也完全是随机的。绝对**不能理解成**各个维度上的残差是不相关的，因为各个维度上的残差其实**是相关的**）

### 2. 加权最小二乘 WLS

**残差不满足Gauss-Markov假设**，因为有些点的权重更高，拟合曲线自然就会更靠近这些点。

并且，我们希望给某些观测值更高的权重，那么我们的目标函数就变成了：
$$
S(\beta)=\sum_{i=1}^m\omega_i|y_i-\hat y_i|^2
$$
为什么要使用这个权重呢？在普通最小二乘法中，我们的因变量方差是一个常数，即：
$$
var(y_i)=\sigma_i^2
$$
假设我们令：
$$
\omega_i=\frac{1}{\sigma_i^2}
$$
来人为设定权重，则可以使WLS比OLS有更小的标准差，从而得到最优的结果。

## 非线性最小二乘 Nonlinear Least Squares

暂略